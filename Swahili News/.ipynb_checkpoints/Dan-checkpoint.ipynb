{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f504f4e3",
   "metadata": {},
   "source": [
    "# Getting news from Tuko News\n",
    "\n",
    "## Schedule\n",
    "\n",
    "- ensure that data mining runs every hour \n",
    "\n",
    "## Request\n",
    "\n",
    "- extract the data from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60f387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e734711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import schedule\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4a0d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_append_data(link, name, c):\n",
    "    '''\n",
    "    fetch_append_data\n",
    "    \n",
    "    A function that collects information from a link and appends it to a csv file\n",
    "    \n",
    "    input : link \n",
    "        a url that has the target information\n",
    "    output: name.csv\n",
    "        a file of the type <name>.csv\n",
    "    '''\n",
    "    url = link\n",
    "    d = r'/'+ c + '/(\\d+)-'\n",
    "    \n",
    "    try:\n",
    "#         fetch content\n",
    "        response = requests.get(url)\n",
    "#     if there is no response or a http error\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser') # parse the html\n",
    "        \n",
    "        data = [] # where the extracted data will be placed \n",
    "        \n",
    "        for news in soup.find_all(\"article\"):\n",
    "            title_tag = news.find('span')\n",
    "            title = title_tag.get_text().strip() if title_tag else \"Title Not Found\"\n",
    "            \n",
    "            image_tag = news.find('img')\n",
    "            image = image_tag['src'] if image_tag else \"Image Not Found\"\n",
    "            \n",
    "            url_tag = news.find('a')\n",
    "            url = url_tag['href'] if url_tag else \"URL Not Found\"\n",
    "            \n",
    "            post_id_match = re.search(d, url)\n",
    "            if post_id_match:\n",
    "                post_id = int(post_id_match.group(1))\n",
    "            else:\n",
    "                post_id = None\n",
    "            time_tag = news.find('time')\n",
    "            time_posted = time_tag['datetime'] if time_tag else \"Time Not Found\"\n",
    "            \n",
    "            data.append([post_id, title, image, url, time_posted])\n",
    "        \n",
    "        \n",
    "#         append to csv\n",
    "        csv_file = str(name) + \".csv\" \n",
    "        with open(csv_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for row in data:\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        print(\"Data appended to CSV successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching or appending data:\", e)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec8bd1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended to CSV successfully.\n"
     ]
    }
   ],
   "source": [
    "samp_url = 'https://www.tuko.co.ke/business-economy/'\n",
    "fetch_append_data(samp_url, 'business', 'business-economy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec84b66",
   "metadata": {},
   "source": [
    "# Adding repeating time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f370bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job():\n",
    "    '''\n",
    "    parameters:\n",
    "    \n",
    "        URL -- the tuko news link you are provided\n",
    "\n",
    "        field -- the name of the csv file or field which data is being scrapped for\n",
    "        \n",
    "        label -- the value between the last forward slash in the url \n",
    "            \n",
    "            /<label>/\n",
    "    '''\n",
    "    field = 'business'\n",
    "    label = 'business-economy'\n",
    "    fetch_append_data(samp_url, field, label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1052a23b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     schedule\u001b[38;5;241m.\u001b[39mrun_pending()\n\u001b[1;32m----> 6\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "schedule.every().hour.do(job)\n",
    "\n",
    "# Run indefinitely\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
